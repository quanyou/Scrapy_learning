Item 对象是种简单的容器，保存爬取到的数据。

声明Item：使用简单的class定义语法以及Field对象来声明。

import scrapy

class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    last_updated = scrapy.Field(serializer=str)


Item字段（Item Fields）
Field对象指明了每个字段的元数据（metadata）。设置Field对象的主要目的就是在一个地方定义好所有的元数据。
那些依赖某个字段的组件肯定是用了特定的键（key）。用来声明item的Field对象并没有被赋值为class的属性，但可以通过Item.fields属性进行访问。

与Item配合

创建item
product = Product(name='Desktop PC', price=1000)
print product

获取字段的值
product['name']

product.get('name')

product['price']

product.get('last_updated', 'not set')

'name' in product


设置字段的值
product['last_updated'] = 'today'


获取所有获取到的值
product.keys()

product.items()


其他任务
    复制item：
    product2 = Product(product)
    print product2

    product3 = product2.copy()
    print product3

   分局item创建字典dict
   dict(product)

   根据字典创建item：
   Product({'name' : 'Laptop PC', 'price' : 15000})

扩展Item ： 通过继承来扩展

Item对象： 额外添加的属性是：fields：一个包含了item所有声明的字段的字典，不仅仅是获取到的字段。该字段的key是字段（field）的名字，值是item声明中使用到的Field对象

字段（Field）对象： 内置的dict类别名，没有额外的方法或属性

Spider参数：
    参数用来定义初始URL或者指定限制爬取网站的部分。




rules : 一个包含一个（或多个）Rule对象的集合（list）。每个Rule对爬取网站的动作定义了特定表现。

爬取规则（Crawling rules）

class scrapy.contrib.spiders.Rule(link_extractor, callback=None,cb_kwargs=None,follow=None,process_links=None,process_request=None)

link_extractor: LinkExtractor 对象，定义如何从爬取到的页面提取链接。

callback: 一个callable或string（该spider中同名的函数将会被调用）。从link_extroctor中每获取到链接时将会调用该函数。接收response为第一个参数，返回一个包含Item以及（或）Request对象（或者这两者的子类）的列表（list）

cb_kwargs: 包含传递给回调函数的参数（keyword argument）的字典。

follow: 布尔值，指定根据该规则从response提取的链接是否需要跟进。如果callback为None， follow默认设置为True，否则默认为False。

process_links: 一个callable或string（该spider中同名的函数将会被调用。）从link_extractor中获取到链接列表时将会调用该函数。（主要用于过滤）

process_request: callable或string（spider中同名的函数将会被调用）。提取到每个requst时都会调用该函数。该函数必须返回一个request或None（用来过滤request）。